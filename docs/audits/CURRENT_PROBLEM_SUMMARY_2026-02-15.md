# Current Problem Summary — Causal Chat + Evidence Rail

**Date:** 2026-02-15 (Asia/Manila)  
**System:** `synthesis-engine` (chat attachment-first pipeline)  
**Primary PDFs used for validation:**
- `Anomaly-Detection.pdf`
- `Disagreement-AI-Alignment.pdf`

---

## Executive Summary

The system has significantly improved (persona control, extraction structure, UI readability), but there are still **critical consistency failures** in final output assembly and rail trust signals.

### Core unresolved failures
1. **Section 2 contradiction**: response sometimes outputs `Section 2: Claim-Eligible Numerics = NONE` even when Section 1 clearly contains candidate metric-bearing numerics.
2. **Category drift**: some metric-like values (e.g., `3` and `15` from “three hours to fifteen minutes”) are misrouted into bibliographic/structural categories in some runs.
3. **Contract leakage**: response occasionally appends extra blocks after Section 3 (`Falsification Criteria`, `Next Epistemic Action`, JSON/scaffold-like tails), violating section-only output contract.
4. **Evidence Rail contradiction**: rail can show `Model Provenance: unavailable` while simultaneously showing additional trust-like model details (`vcustom • Active Reasoner`, `Hash: verified`), which is internally inconsistent.

These are now the main blockers to trustworthy production signoff.

---

## What is already fixed (working improvements)

## 1) Persona hardening
- Legacy Taoist/philosophical style leakage was repeatedly patched and mostly eliminated in normal runs.
- Additional anti-persona sanitization patterns were added to remove poetic/legacy phrases from chat output.

## 2) Incomplete Claude response handling
- Anthropic adapter now supports:
  - multi-block text assembly
  - controlled one-hop continuation on `max_tokens` stop reason

## 3) Numeric extraction progression
- Pipeline now supports:
  - broader explicit numeric capture
  - word-number support (e.g., `three`, `fifteen`)
  - duration/scale phrase extraction (`three hours to fifteen minutes`, `billions of events`, `petabytes`)
- Reduced prior failures:
  - DOI fragment artifacts
  - alphanumeric token leakage (`M3`, `EC2` as fake metrics)

## 4) Output readability improvements
- Grouped categories and cleaner list formatting implemented.
- Snippet clipping artifacts reduced (`le.`, `gineering` style fragments).
- Section headers and key labels made visually stronger.
- Empty markdown-fence rendering artifact patched (blank rounded block under prediction/test content).

## 5) Evidence Rail UI improvements
- Better visual hierarchy and confidence display.
- Fallback display behavior improved in several places.

---

## Current observed failures in detail

## A. Section-level logical inconsistency

### Symptom
The model outputs:
- Section 1 includes potential metric-like entries (`1000`, `1,000,000,000`, `3`, `15`), **but**
- Section 2 says `NONE`.

### Why this is severe
This breaks deterministic trust expectations and makes downstream claims unreliable.

### Likely root cause
Section 2 appears to be generated by model narrative discretion rather than a strict deterministic derivation from classified Section 1 buckets.

---

## B. Category instability for the same source phrase

### Symptom
`3` and `15` from the same RCA phrase are sometimes classified as non-metric (`bibliographic`), despite being temporal delta descriptors.

### Why this is severe
Category instability across runs undermines reproducibility and claim-gating confidence.

### Likely root cause
Classifier precedence and context-window interpretation are still too permissive/fragile for mixed prose + bibliographic neighborhoods.

---

## C. Section-only contract still not airtight

### Symptom
Output still includes extra sections beyond mandated Section 1/2/3:
- `Falsification Criteria`
- `Next Epistemic Action`
- long planning tails

### Why this is severe
Violates declared output contract and creates UI/UX inconsistency.

### Likely root cause
Current post-generation contract gate is pattern-based and can miss variants of trailing blocks.

---

## D. Evidence Rail trust contradiction

### Symptom
`Model Provenance: unavailable` appears with trust-signaling details that imply known/verified provenance.

### Why this is severe
This directly harms trust semantics: “unavailable” must not coexist with “verified” details.

### Likely root cause
Separate UI subfields are populated from partially independent states/events without a hard consistency guard.

---

## E. Follow-up memory/continuity behavior (user-observed)

### Symptom
On follow-up prompt (“What was the title of the PDF I just uploaded?”), response acted as if prior extracted context/title was not reliably available.

### Why this is severe
Practical conversational continuity appears broken from user perspective.

### Likely root cause
Attachment ingestion artifacts/metadata are not consistently bound and reused for direct follow-up retrieval in all paths.

---

## Product impact

## Trust impact
- Contradictory sections and rail state create “looks smart but inconsistent” behavior.

## UX impact
- Users must manually cross-check model statements in the same response.

## Governance impact
- Hard-gate architecture intent (deterministic + provenance-first + graceful degradation) is partially met, but consistency gates are still bypassable at final assembly.

---

## Required closure criteria (GO conditions)

## 1) Deterministic section coherence
- Section 2 must be computed directly from Section 1 category outputs (`potential_metric` bucket), not freeform model inference.
- If Section 2 = NONE, no claim may cite metric-bearing numerics.

## 2) Hard output truncation
- Final response must contain only Section 1, Section 2, Section 3 (plus required evidence-class line inside Section 3).
- Any trailing blocks must be dropped unconditionally.

## 3) Stable category classification
- Add explicit precedence + test fixtures for:
  - temporal delta phrases (`X hours to Y minutes`) => metric-bearing
  - scale quantifiers (`billions of events`, `petabytes`) => metric-bearing (with low/medium confidence)
  - section references (`§3.2`) => structural

## 4) Evidence Rail integrity guard
- If model provenance is unavailable, hide or disable all provenance-verification sublines.
- Domain card should show only verified emitted domain for current run; otherwise `unavailable`.

## 5) Follow-up retrieval continuity
- For same session, last ingested attachment metadata/title must be queryable by follow-up prompts without re-upload.

---

## Recommended immediate patch order

1. **Server-side deterministic section assembler** (highest priority)  
   Build Section 2 from Section 1 `potential_metric` list in code before final emission.

2. **Strict final-output sanitizer**  
   Unconditionally cut output after Section 3 block.

3. **Classifier precedence lock**  
   Add and enforce non-overlapping rule order for structural vs metric-bearing cues.

4. **Evidence Rail consistency guard**  
   Prevent contradictory provenance states in UI.

5. **Attachment-memory carryover for follow-up**  
   Ensure title/name retrieval from recent ingestion metadata.

---

## Final status

**Status: IN PROGRESS (not final GO yet)**

The system has achieved substantial improvements and is close, but still fails strict trust criteria due to section contradiction and rail consistency issues.
